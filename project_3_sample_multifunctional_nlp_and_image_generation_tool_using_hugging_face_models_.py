# -*- coding: utf-8 -*-
"""Project 3 sample_Multifunctional NLP and Image Generation Tool  using Hugging Face Models .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IVLPT4txbGn7XJKlFkNWUp1PzsAFmyjx

Text Summarization
"""

!pip install transformers torch

from transformers import BartForConditionalGeneration, BartTokenizer
import torch

# Load pre-trained model and tokenizer
model_name = "facebook/bart-large-cnn"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

# Set the model to evaluation mode
model.eval()

def summarize(text, model, tokenizer, max_length=130, min_length=30, do_sample=False):
    inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Example text
text = """
Hugging Face Inc. is a company that develops tools for building applications using machine learning. Its headquarters are in New York City.
It was founded in 2016 by Clement Delangue, Julien Chaumond, and Thomas Wolf. Initially, Hugging Face created a chatbot app targeted at teenagers.
In 2019, Hugging Face transformed into an open-source provider of NLP technologies.
Its model library, Transformers, became widely used in both academia and industry,
garnering significant attention for its implementation of state-of-the-art NLP models.
"""

# Perform summarization
summary = summarize(text, model, tokenizer)
print("Original text:\n", text)
print("\nSummary:\n", summary)

"""Next Word Prediction"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Set the model to evaluation mode
model.eval()

import torch

def predict_next_word(prompt, model, tokenizer, top_k=5):
    # Tokenize input text
    inputs = tokenizer(prompt, return_tensors='pt')

    # Generate predictions
    with torch.no_grad():
        outputs = model(**inputs)

    # Get logits of the last token in the sequence
    next_token_logits = outputs.logits[:, -1, :]

    # Filter the top k tokens by their probability
    top_k_tokens = torch.topk(next_token_logits, top_k).indices[0].tolist()

    # Decode the top k tokens to their corresponding words
    predicted_tokens = [tokenizer.decode([token]) for token in top_k_tokens]

    return predicted_tokens

# Example usage
prompt = "on a rainy day, I want to eat"
predicted_words = predict_next_word(prompt, model, tokenizer)
print(f"Prompt: {prompt}")
print(f"Next word predictions: {predicted_words}")

"""Story Prediction"""

def generate_text(prompt, model, tokenizer, max_length=50):
    generated = tokenizer.encode(prompt, return_tensors='pt')

    model.eval()
    with torch.no_grad():
        for _ in range(max_length):
            outputs = model(generated)
            next_token_logits = outputs.logits[:, -1, :]
            next_token = torch.argmax(next_token_logits, dim=-1)
            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)
            if next_token == tokenizer.eos_token_id:
                break

    return tokenizer.decode(generated[0], skip_special_tokens=True)

# Example usage
prompt = "on a rainy day, I want to eat"
generated_text = generate_text(prompt, model, tokenizer)
print(f"Generated text: {generated_text}")

"""Chatbot"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load pre-trained model and tokenizer with padding set to left
model_name = "microsoft/DialoGPT-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')
model = AutoModelForCausalLM.from_pretrained(model_name)

# Set the model to evaluation mode
model.eval()

def chat_with_model(prompt, chat_history_ids, model, tokenizer):
    # Encode the new user input, add the eos_token and return a tensor in PyTorch
    new_user_input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')

    # Append the new user input tokens to the chat history
    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if chat_history_ids is not None else new_user_input_ids

    # Generate a response
    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)

    # Decode the response and print it
    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)

    return response, chat_history_ids

# Initialize chat history
chat_history_ids = None

print("Start chatting with the bot (type 'quit' to stop)")

while True:
    # Get user input
    user_input = input("You: ")

    # Exit condition
    if user_input.lower() == 'quit':
        break

    # Get the response from the model
    response, chat_history_ids = chat_with_model(user_input, chat_history_ids, model, tokenizer)

    print(f"Bot: {response}")

"""Sentiment Analysis"""

from transformers import pipeline

# Load pre-trained sentiment-analysis pipeline
sentiment_analysis = pipeline("sentiment-analysis")

def analyze_sentiment(texts):
    results = sentiment_analysis(texts)
    for text, result in zip(texts, results):
        print(f"Text: {text}")
        print(f"Sentiment: {result['label']}, Score: {result['score']:.4f}")

# Example texts for sentiment analysis
example_texts = [
    "I love using Hugging Face's transformers library!",
    "The weather is gloomy and I feel sad.",
    "I'm so excited for the new movie release!",
    "I'm feeling quite neutral about this event."
]

# Perform sentiment analysis
for text in example_texts:
  analyze_sentiment(text)

"""Question Answering"""

from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

# Load the pre-trained model and tokenizer
model_name = "bert-large-uncased-whole-word-masking-finetuned-squad"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

def answer_question(question, context):
    # Tokenize input question and context
    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors="pt")

    # Get input ids and attention mask
    input_ids = inputs["input_ids"].tolist()[0]

    # Get the model's output
    outputs = model(**inputs)
    answer_start_scores = outputs.start_logits
    answer_end_scores = outputs.end_logits

    # Get the most likely beginning and end of answer with the argmax of the score
    answer_start = torch.argmax(answer_start_scores)
    answer_end = torch.argmax(answer_end_scores) + 1

    # Convert tokens to the answer
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))

    return answer

# Define a context and a question
context = """
The Transformers library by Hugging Face provides general-purpose architectures for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with pretrained models in over 100 languages.
These models can be used on a variety of tasks, including text classification, information extraction, question answering, summarization, translation, and text generation.
"""

question = "What does the Transformers library provide?"

# Get the answer
answer = answer_question(question, context)
print(f"Question: {question}")
print(f"Answer: {answer}")

"""Image Generation"""

# Install the necessary libraries
!pip install diffusers transformers torch

import torch
from diffusers import StableDiffusionPipeline

# Load the pre-trained Stable Diffusion model
model_id = "CompVis/stable-diffusion-v1-4"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

def generate_image(prompt, pipe):
    with torch.no_grad():
        image = pipe(prompt).images[0]
    return image

# Example text prompt
prompt = "A futuristic cityscape at sunset"

# Generate image
image = generate_image(prompt, pipe)

# Display the generated image
image.show()

# Example text prompt
prompt = "Teaching Transformers for students"

# Generate image
image = generate_image(prompt, pipe)

# Display the generated image
image.show()

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
imgplot = plt.imshow(image)
plt.show()